{
  "AWSTemplateFormatVersion" : "2010-09-09",
  "Description" : "Build an EC2 instances to test Docker on AWS - DON'T use for 100's of instances",
  "Parameters" : {
    "KeyName" : {
      "Description" : "Name of an existing EC2 KeyPair to enable SSH access to the instance",
      "Type" : "AWS::EC2::KeyPair::KeyName",
      "ConstraintDescription" : "Can contain only ASCII characters."
    },
    "InstanceType" : {
      "Description" : "EC2 instance type",
      "Type" : "String",
      "Default" : "i2.xlarge",
      "AllowedValues" : [
        "i2.xlarge",
        "i2.2xlarge",
        "i2.4xlarge"
      ],
      "ConstraintDescription" : "Must be a valid EC2 instance type"
    },
    "SpotPrice" : {
      "Description" : "Spot price",
      "Type" : "Number",
      "MinValue" : "0.01",
      "MaxValue" : "5.00"
    },
    "DockerPassword" : {
      "Description" : "Docker password",
      "Type" : "String"
    },
    "Owner" : {
      "Description" : "Who is running this",
      "Type" : "String"
    },
    "Instances" : {
      "Description" : "The number of Workers",
      "Type" : "Number",
      "MinValue" : "1",
      "Default"  : "1",
      "ConstraintDescription" : "Enter a number >= 1"
    },
    "AmiId" : {
      "Description" : "Which AMI should we use",
      "Type" : "AWS::EC2::Image::Id",
      "ConstraintDescription" : "Must be a valid AMI Id",
      "Default" : "ami-f6856596"
    }
  },
  "Resources" : {
    "WorkerSecurityGroup" : {
      "Type" : "AWS::EC2::SecurityGroup",
      "Properties" : {
        "GroupDescription" : "Enable SSH access via port 22",
        "VpcId" : "vpc-9e6622fb",
        "SecurityGroupIngress" : [
          {
            "IpProtocol" : "tcp",
            "FromPort" : "22",
            "ToPort" : "22",
            "CidrIp" : "0.0.0.0/0"
          },
          {
            "IpProtocol" : "tcp",
            "FromPort" : "8000",
            "ToPort" : "8000",
            "CidrIp" : "0.0.0.0/0"
          },
          {
            "IpProtocol" : "tcp",
            "FromPort" : "8001",
            "ToPort" : "8001",
            "CidrIp" : "0.0.0.0/0"
          },
          {
            "IpProtocol" : "tcp",
            "FromPort" : "4000",
            "ToPort" : "4000",
            "CidrIp" : "0.0.0.0/0"
          },
          {
            "IpProtocol" : "tcp",
            "FromPort" : "32000",
            "ToPort" : "64000",
            "CidrIp" : "0.0.0.0/0"
          }
        ]
      }
    },
    "CfnUser" : {
      "Type" : "AWS::IAM::User",
      "Properties" : {
        "Path" : "/",
        "Policies" : [
          {
            "PolicyName" : "Admin",
            "PolicyDocument" : {
              "Statement" : [
                {
                  "Effect" : "Allow",
                  "Action" : "*",
                  "Resource" : "*"
                }
              ]
            }
          }
        ]
      }
    },
    "HostKeys" : {
      "Type" : "AWS::IAM::AccessKey",
      "Properties" : {
        "UserName" : {
          "Ref" : "CfnUser"
        }
      }
    },
    "AutoScalingGroup" : {
      "Type" : "AWS::AutoScaling::AutoScalingGroup",
      "Properties" : {
        "AvailabilityZones" : {
          "Fn::GetAZs" : ""
        },
        "LaunchConfigurationName" : {
          "Ref" : "LaunchConfig"
        },
        "VPCZoneIdentifier" : [
          "subnet-04e68e73",
          "subnet-bde9a3d8",
          "subnet-628b063b"
        ],
        "DesiredCapacity" : {"Ref" : "Instances"} ,
        "MinSize" : 1,
        "MaxSize" : {"Ref" : "Instances"},
        "Tags": [
          {"PropagateAtLaunch": true, "Key" : "Name", "Value" : "DFMS Instance"},
          {"PropagateAtLaunch": true, "Key" : "Owner", "Value" : { "Ref" : "Owner" }}
        ]
      }
    },
    "LaunchConfig" : {
      "Type" : "AWS::AutoScaling::LaunchConfiguration",
      "Metadata" : {
        "Comment" : "Create a new instance",
        "AWS::CloudFormation::Init" : {
          "config" : {
            "packages" : {
              "yum" : {
                "htop" : [],
                "sysstat" : [],
                "iotop" : []
              }
            },
            "files" : {
              "/etc/sysconfig/docker": {
                "content": {
                  "Fn::Join": [
                    "",
                    [
                      "# The max number of open files for the daemon itself, and all\n",
                      "# running containers.  The default value of 1048576 mirrors the value\n",
                      "# used by the systemd service unit.\n",
                      "DAEMON_MAXFILES=1048576\n",
                      "\n",
                      "# Additional startup options for the Docker daemon, for example:\n",
                      "# OPTIONS=\"--ip-forward=true --iptables=true\"\n",
                      "# By default we limit the number of open files per container\n",
                      "OPTIONS=\"-D --default-ulimit nofile=1024:4096\"\n"
                    ]
                  ]
                },
                "mode": "000644",
                "owner": "root",
                "group": "root"
              },
              "/etc/sysconfig/docker-storage-setup": {
                "content": {
                  "Fn::Join": [
                    "",
                    [
                      "VG=dfms-group\n",
                      "DATA_SIZE=100GB\n"
                    ]
                  ]
                }
              },
              "/home/ec2-user/.aws/credentials" : {
                "content" : {
                  "Fn::Join" : [
                    "",
                    [
                      "[aws-chiles02]\n",
                      "aws_access_key_id = ", { "Ref" : "HostKeys" }, "\n",
                      "aws_secret_access_key = ", {"Fn::GetAtt" : ["HostKeys", "SecretAccessKey"]}, "\n"
                    ]
                  ]
                },
                "mode"   : "000544",
                "owner"  : "root",
                "group"  : "root"
              },
              "/root/.aws/credentials" : {
                "content" : {
                  "Fn::Join" : [
                    "",
                    [
                      "[aws-chiles02]\n",
                      "aws_access_key_id = ", { "Ref" : "HostKeys" }, "\n",
                      "aws_secret_access_key = ", {"Fn::GetAtt" : ["HostKeys", "SecretAccessKey"]}, "\n"
                    ]
                  ]
                },
                "mode"   : "000544",
                "owner"  : "root",
                "group"  : "root"
              }
            }
          }
        }
      },
      "Properties" : {
        "ImageId" : { "Ref" : "AmiId" },
        "AssociatePublicIpAddress" : "true",
        "BlockDeviceMappings" : {
          "Fn::FindInMap" : [
            "EphemeralDisks",
            { "Ref": "InstanceType" },
            "BlockDeviceMappings"
          ]
        },
        "SpotPrice" : {
          "Ref" : "SpotPrice"
        },
        "InstanceType" : {
          "Ref" : "InstanceType"
        },
        "SecurityGroups" : [ { "Ref" : "WorkerSecurityGroup" } ],
        "KeyName" : {
          "Ref" : "KeyName"
        },
        "UserData" : {
          "Fn::Base64" : {
            "Fn::Join" : [
              "",
              [
                "#!/bin/bash -ve\n",
                "yum -y update\n",

                "# Run cfn-init\n",
                "/opt/aws/bin/cfn-init -v ",
                "         --stack ",
                { "Ref" : "AWS::StackName" },
                "         --resource LaunchConfig ",
                "         --region ",
                { "Ref" : "AWS::Region" },
                "\n",
                "# Print into the logs the disk free\n",
                "\n",
                "df -h\n",
                "\n",
                "# Move the docker volumes to the ephemeral drive\n",
                "service docker stop\n",
                "sleep 10\n",
                "\n",
                "if [ -b \"/dev/xvdb\" ]; then\n",
                "\n",
                "    METADATA_URL_BASE=\"http://169.254.169.254/latest\"\n",
                "\n",
                "    yum -y -d0 install docker-storage-setup curl\n",
                "\n",
                "    # Configure Raid if needed - taking into account xvdb or sdb\n",
                "    root_drive=`df -h | grep -v grep | awk 'NR==2{print $1}'`\n",
                "\n",
                "    if [ \"$root_drive\" == \"/dev/xvda1\" ]; then\n",
                "      echo \"Detected 'xvd' drive naming scheme (root: $root_drive)\"\n",
                "      DRIVE_SCHEME='xvd'\n",
                "    else\n",
                "      echo \"Detected 'sd' drive naming scheme (root: $root_drive)\"\n",
                "      DRIVE_SCHEME='sd'\n",
                "    fi\n",
                "\n",
                "    # figure out how many ephemerals we have by querying the metadata API, and then:\n",
                "    #  - convert the drive name returned from the API to the hosts DRIVE_SCHEME, if necessary\n",
                "    #  - verify a matching device is available in /dev/\n",
                "    drives=\"\"\n",
                "    ephemeral_count=0\n",
                "    ephemerals=$(curl --silent $METADATA_URL_BASE/meta-data/block-device-mapping/ | grep ephemeral)\n",
                "    for e in $ephemerals; do\n",
                "      echo \"Probing $e ..\"\n",
                "      device_name=$(curl --silent $METADATA_URL_BASE/meta-data/block-device-mapping/$e)\n",
                "      # might have to convert 'sdb' -> 'xvdb'\n",
                "      device_name=$(echo $device_name | sed \"s/sd/$DRIVE_SCHEME/\")\n",
                "      device_path=\"/dev/$device_name\"\n",
                "\n",
                "      # test that the device actually exists since you can request more ephemeral drives than are available\n",
                "      # for an instance type and the meta-data API will happily tell you it exists when it really does not.\n",
                "      if [ -b $device_path ]; then\n",
                "        echo \"Detected ephemeral disk: $device_path\"\n",
                "        drives=\"$drives $device_path\"\n",
                "        ephemeral_count=$((ephemeral_count + 1 ))\n",
                "      else\n",
                "        echo \"Ephemeral disk $e, $device_path is not present. skipping\"\n",
                "      fi\n",
                "    done\n",
                "\n",
                "    echo \"ephemeral_count = $ephemeral_count\"\n",
                "    if (( ephemeral_count >= 1 )); then\n",
                "      if mountpoint -q \"/media/ephemeral0\" ; then\n",
                "        umount /media/ephemeral0\n",
                "      fi\n",
                "      # overwrite first few blocks in case there is a filesystem, otherwise mdadm will prompt for input\n",
                "      for drive in $drives; do\n",
                "        dd if=/dev/zero of=$drive bs=4096 count=1024\n",
                "      done\n",
                "\n",
                "      if (( ephemeral_count > 1 )); then\n",
                "        mdadm --create --verbose /dev/md0 --level=0 -c256 --raid-devices=$ephemeral_count $drives\n",
                "        blockdev --setra 65536 /dev/md0\n",
                "        pvcreate /dev/md0\n",
                "        vgcreate dfms-group /dev/md0\n",
                "      else\n",
                "        pvcreate $drives\n",
                "        vgcreate dfms-group $drives\n",
                "      fi\n",
                "      lvcreate -L 10G --name swap dfms-group\n",
                "      docker-storage-setup\n",
                "      lvcreate --extents 100%FREE --name data dfms-group\n",
                "\n",
                "      mkfs.xfs -K /dev/dfms-group/data\n",
                "      mkdir -p /mnt/daliuge\n",
                "      mount /dev/dfms-group/data /mnt/daliuge\n",
                "\n",
                "      mkswap /dev/dfms-group/swap\n",
                "      swapon /dev/dfms-group/swap\n",
                "    else\n",
                "      mkdir -p /mnt/daliuge\n",
                "      mkfs.xfs -K /dev/xvdb\n",
                "\n",
                "      mount /dev/xvdb /mnt/daliuge\n",
                "      dd if=/dev/zero of=/mnt/swapfile bs=1M count=1024\n",
                "      mkswap /mnt/swapfile\n",
                "      swapon /mnt/swapfile\n",
                "      chmod 0600 /mnt/swapfile\n",
                "    fi\n",
                "fi\n",
                "# Print free disk space\n",
                "df -h\n",
                "\n",
                "# Create the DFMS root\n",
                "mkdir -p /mnt/daliuge/dfms_root\n",
                "chmod -R 0777 /mnt/daliuge\n",
                "\n",
                "rm -rf /var/lib/docker\n",
                "service docker start\n",
                "sleep 10\n",
                "\n",
                "#docker login --email=a@b.com --username=icrar --password=",
                { "Ref" : "DockerPassword" },
                " sdp-docker-registry.icrar.uwa.edu.au:8080\n",
                "\n",
                "# Get the docker containers now to prevent a race condition later\n",
                "#docker pull sdp-docker-registry.icrar.uwa.edu.au:8080/kevin/java-s3-copy:latest\n",
                "#docker pull sdp-docker-registry.icrar.uwa.edu.au:8080/kevin/chiles02:latest\n",
                "docker pull kevinvinsen/java-s3-copy:latest\n",
                "docker pull kevinvinsen/chiles02:latest\n",
                "\n",
                "cd /home/ec2-user\n",
                "runuser -l ec2-user -c 'cd /home/ec2-user/dfms && git pull'\n",
                "runuser -l ec2-user -c 'cd /home/ec2-user/dfms && source /home/ec2-user/virtualenv/dfms/bin/activate && python setup.py install'\n",
                "runuser -l ec2-user -c 'cd /home/ec2-user && git clone https://github.com/ICRAR/aws-chiles02.git'\n",
                "\n",
                "cat /home/ec2-user/.ssh/id_dfms.pub >> /home/ec2-user/.ssh/authorized_keys\n",
                "runuser -l ec2-user -c 'cd /home/ec2-user/aws-chiles02/pipeline/aws_chiles02 && source /home/ec2-user/virtualenv/aws-chiles02/bin/activate && python startup_complete.py startup_complete us-west-2'\n",
                "runuser -l ec2-user -c 'cd /home/ec2-user/dfms && source /home/ec2-user/virtualenv/dfms/bin/activate && nohup dfmsNM --rest -v --dfms-path=/home/ec2-user/aws-chiles02/pipeline --id=kv -H 0.0.0.0 > /mnt/daliuge/dfms_root/logfile.log 2>&1 &'\n",
                "#runuser -l ec2-user -c 'cd /home/ec2-user/dfms && source /home/ec2-user/virtualenv/dfms/bin/activate && dfmsNM --rest -v --dfms-path=/home/ec2-user/aws-chiles02/pipeline --id=kv -H 0.0.0.0'\n",
                "#dfmsDIM --rest -v --id=kv -H 0.0.0.0 --ssh-pkey-path ~/.ssh/id_dfms --nodes NODES\n"
              ]
            ]
          }
        }
      }
    }
  },
  "Mappings" : {
    "EphemeralDisks" : {
      "i2.xlarge" : {
        "BlockDeviceMappings" : [
          {
            "DeviceName" : "/dev/sdb",
            "VirtualName" : "ephemeral0"
          }
        ]
      },
      "i2.2xlarge" : {
        "BlockDeviceMappings" : [
          {
            "DeviceName" : "/dev/sdb",
            "VirtualName" : "ephemeral0"
          },
          {
            "DeviceName" : "/dev/sdc",
            "VirtualName" : "ephemeral1"
          }
        ]
      },
      "i2.4xlarge" : {
        "BlockDeviceMappings" : [
          {
            "DeviceName" : "/dev/sdb",
            "VirtualName" : "ephemeral0"
          },
          {
            "DeviceName" : "/dev/sdc",
            "VirtualName" : "ephemeral1"
          },
          {
            "DeviceName" : "/dev/sdd",
            "VirtualName" : "ephemeral2"
          },
          {
            "DeviceName" : "/dev/sde",
            "VirtualName" : "ephemeral3"
          }
        ]
      }
    }
  }
}

